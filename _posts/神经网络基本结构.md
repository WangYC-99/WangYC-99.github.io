# [neural network]神经网络基本结构

> by WangYC_99
>
> @home Feb.15th 2022

## 1. 结构名词解释

$$
神经网络
\begin{cases}
convolutional\ neural\ network:卷积神经网络 \ 多用于图像识别
\\ 
\\Long\ short-term\ memory\ network:长短期记忆网络 \ 多用于语音识别
\end{cases}
$$

Neuron: 神经元 Thing that hold a number，数字容器

Activation: 激活值 图像神经网络中表示灰度值或者其他的概率等

Hidden Layers: 隐藏层 

Bias 能够达到激活标准的最小值（前一层的激活值先经过加权求和后加上这个bias后再送入激活函数中进行计算）。

### 2. 激活函数的作用

上一层的n个激活值通过加权求和以后得到一个值，将这个值对应到[0, 1]之间得到新的激活值，激活函数的作用就是在这里面起到归一化的作用。

### 3. 损失函数以及梯度下降算法的思路

将网络各层之间的参数作为一个输入，衡量评估多次训练的网络的效果，定义为一个函数，就叫做损失函数(cost-function)。

以一个变量为输入的函数为例，在变量初始位置求斜率（梯度），沿着梯度的相反方向改变输入值就能得到向着损失函数值减小的方向改变的方法，这样来迭代改变权重以及偏执，最终得到最好的训练效果。

损失函数的计算以及利用损失函数调整权重的值的过程是整个神经网络的核心，叫做反向传播(BP backpropagation)。

### 4. 梯度下降反向传播的操作过程

以两层的数字识别为例子，第一层为图像各个像素抽象出来的tensor，第二层为各个结果的概率。

当我们拿出训练数据集中的2来的时候，我们想要让第一层读入的tensor能够最大化得激活第二层的2这个结果，同时弱化对于其他结果的激活，这时我们就需要做出调整，能够想到的调整一共有三种，分别是1. 改变权重的大小，2. 改变第一层激活值的大小还有3. 改变bias。其中1和2是相辅相成的，能够表达出来训练样本希望如何调整权重，以及调整某些函数的力度是多少才能够让梯度更快地下降。利用这个思路，我们能够得到一个2的训练数据希望对目前的bias和weight的调整的愿望。（强化第二层中2的激活，弱化其他层的激活）

累计多个训练数据，求平均，即可得到对于权重以及激活值的调整的愿望。

### 5. 训练的问题以及解决方案

如果上述的训练过程的每一步都采用整个数据集进行训练的话，训练的成本代价会很大。因此可以采用随机梯度下降的方法进行，也就是

### 6. 简单神经网络的缺陷

简单多层感知器构成的神经网络并不具备区分抵抗噪声的能力，也就是虽然网络具有能够识别标准输入为正确答案的能力，但是不具备将不标准输入从标准输入中区分出来的能力。