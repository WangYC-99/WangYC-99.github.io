---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am a Ph.D. student at the School of Information, Renmin University of China, under supervision of [Jing Zhang](https://xiaojingzi.github.io/). 
At the same time, I'm an intern at KEG of Tsinghua University and ZHIPU AI, under supervision of [Jifan Yu](https://yujifan0326.github.io/). 
Privious to that, I graduated from [Honors College](https://honors.nwpu.edu.cn/) of Northwestern Polytechnical University, where I majored in Computer Science. 
Currently, my research focuses on Tool Intelligence of LLMs and AI for Education.

# Publications

## 2023

### Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method
Shicheng Tan, Weng Lam Tam, **Yuanchun Wang**, Wenwen Gong, Shu Zhao, Peng Zhang, Jie Tang
[code](https://github.com/aitsc/GLMKD), [PDF](https://aclanthology.org/2023.findings-acl.614.pdf)
<div style="display: flex; align-items: center;">
    <div style="flex: 1;">
        ![main pic](../images/glmd.jpg)
    </div>
    <div style="flex: 2;">
        A general language model distillation (GLMD) method that performs two-stage word prediction distillation and vocabulary compression, which is simple and surprisingly shows extremely strong performance.
    </div>
</div>